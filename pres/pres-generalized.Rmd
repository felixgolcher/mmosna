---
title: "From general to generalized (linear) mixed models"
subtitle: "Non-normally distributed data"
author: '[Felix Golcher](https://www.linguistik.hu-berlin.de/de/staff/golcherf) [\@fgolcher](https://twitter.com/fgolcher/)'
date: "2021-10-29"
bibliography: [flybib.bib,bibliography.bib]
tables: yes
output:
  iosp::ioslides_plus:
    fig_height: 5
    fig_width: 6
    keep_md: no
    logo: fig/hu.png
    footer: "R Workshop, 2021-10-29 Session 1"
    smaller: yes
    widescreen: yes
    self_contained: false
    lib_dir: libs
    fig_retina: null
    css: styles.css
---

```{r config, include=FALSE}
pcks <- c("ggplot2", "tidyverse","lme4","languageR", "itsadug", 
          "car", "kableExtra",
          "ggrepel","broom", "latex2exp","lmerTest", "knitr", "rmarkdown", "performance")
```


```{r, include=FALSE}
## ähnliches geht auch mit write_bib {knitr}
## die anreicherung muss dann natürlich auch extra durchgeführt werden.
## pcks list of packages.
for(pck in pcks) library(pck, character.only = T)
pcks <- unique(c(pcks, names(sessionInfo()$otherPkgs)))
versions <- installed.packages()[pcks,"Version"]
fc <- file("flybib.bib", open="wt")
invisible(sapply(pcks, function(pck){
  pck.cit <- citation(pck)[[1]]
  pck.cit$key <- pck
  hds <- ifelse(pck == "base", 
                "Version", 
                paste("R package \\texttt{",pck,"} version",sep=""))
  vsn <- paste(hds,versions[pck])
  if(length(grep("version",pck.cit$note))==0){
      pck.cit$addendum <- ifelse(is.null(pck.cit$note), 
                           vsn, 
                           paste(pck.cit$note,vsn,sep=", "))
  }
  writeLines(toBibtex(pck.cit),fc)
}))
close(fc)
```

```{r knitroptions, include=FALSE}
knitr::opts_chunk$set(cache=T,
                      #echo=FALSE,
                      #results='hide',
                      message=FALSE,
                      warning=FALSE,
                      autodep=T,
                      root.dir=getwd()
               )
theme_set(theme_minimal())# +
```


## where are we | and where do we wanna go?

### where we are {.box .bg-green .build}

* We know how to handle data that are
  - normally (gaussianly(?)) distributed
  - or can be tranformed into something normally distributed
* The **mean** expectation value might vary depending on many variables on different levels.
* We allow for idiosyncratic variation between people, items, plots of land, years, countries...
  - The expectation value might be distributed normally around the **mean value**    (random intercept)
  - The effect of each variable might be normally distributed around it's mean value (random slope)

### What's lying ahead? {.box .bg-red .build}

* Not all data [are/is/does](https://www.theguardian.com/news/datablog/2010/jul/16/data-plural-singular) normally distributed.
  - actually, no data is.
* If your data follow one of several standard distributions you can extend the models we know to them.
  - **Binomially** distributed data: how many out of $n$ times did something happen?
  - **Poisson** data: How often did something happen in a given time?
  - Other distributions, most notably the **Gamma** distribution.
* Then you do not directly model the expectation value anymore.

## What is a Poisson distribution? {.build}

```{r, include=FALSE}
lambda <- .45
```
### {.box-9}

* Suppose you stand at a traffic light. It is a very quiet street.
* You count the numbers of cars stopped by each red light.
* Usually it's zero. Sometimes there is one car, somtimes two cars etc.
* Let's suppose, there are $\lambda = `r lambda`$ cars on average.

### The PMF {.box-3}

$\Pr(k)= \frac{\lambda^k e^{-\lambda}}{k!}$

%end%

```{r, title="That's what you see: The Poisson distribution", row=c(6,5), fig.width=3, fig.height=1.5}
data.frame(cars = 0:10) %>% 
  mutate(probability = dpois(cars, lambda)) %>% 
  filter(probability>.001) %>% 
  ggplot(aes(cars, probability)) + 
  geom_bar(stat = "identity")
```

### {.box-12}

* This distribution is completely defined by the mean $\lambda$.
* The variance is always $\lambda$ as well. The Poisson distribution has only one parameter!

## Set up example data | simulation {.build}

```{r, include=FALSE}
rate <- .1 # pro s => this gives us the log, so not per second!
eff <- c(0.,.05) # delta pro s
mn <- 6
mx <- 12
time <- seq(mn, mx, length=2000)  # s
```


> * Suppose we install some wild life camera in the forest.
* It is triggered `r signif(exp(rate),4)` times per hour.
* Not all nights have are equally long, but have between `r mn` and `r mx` hour.
* We watch the same habitat again after 10 years.
* We want to figure out if something changed. 
* Let's assume it actually did and the rate is now `r signif(exp(eff[2]),4)` times as high, that is `r signif(exp(rate+eff[2]),4)` per hour.

$$
\text{some outcome} = \text{some offset} + \text{some effects of some variables}
$$
$$
\eta = \eta_0 + a\cdot when
$$

> * Here is $\eta_0$ the baserate of events.
* $when$ is 0 for measurements 10 years ago, 1 for measurements taken this year.
* $a$ is some parameter. It would be 0 if nothing changed.

## The link | math magic {.build}

> * For a gausian model, $\eta$ would be the expectation value $E(y)$ of the response $y$.
* The response itself would be normally distributed around it.
* Here, it is some function of the expectation value:

$$
\lambda = E(y) = h(\eta) = h(\eta_0 + a\cdot when)
$$

> * For Poisson data we usually use $h = exp$. That gives

$$
\lambda = E(y) = e^\eta = e^{\eta_0 + a\cdot when} = e^{\eta_0}e^{a\cdot when}
$$

> * Or the other way round: $\eta = \log\eta$.
* That is, if we assume, something happens `r signif(exp(rate), 3)` every hour
  $$\eta_0 = \log(`r signif(exp(rate), 4)`) = `r signif(rate,4)`$$
* and when it now happens `r signif(exp(eff[2]), 3)` times so often than 10 years ago
  $$a = \log(`r signif(exp(eff[2]), 4)`) = `r signif(eff[2],4)`$$

## One last step | the length of a night

* The number of trigger events will be proportional to the length $L$ of the night.
* So what we have up to now has to be modified to accommodate our situation:

$$
\lambda = E(y) = L\cdot e^\eta = L\cdot e^{\eta_0 + a\cdot when} = L\cdot e^{\eta_0}e^{a\cdot when}
$$


* or, the other way round:


$$
\log \lambda = \log( L\cdot e^\eta) = \log( L\cdot e^{\eta_0}e^{a\cdot when}) = \log L + \eta_0 + a\cdot when
$$

* $\log L$ is called an *offset term*
* Let's remember that $\lambda$ is the expectation value of the trigger events in one night.
* The trigger events will be Poisson distributed around this value.

## Why do we do this? {.build}

* In this form, it's mathematically manageable.
* Some influence, say climate change, will more likely have a relative, multiplicative influence:
  - in one place there where around 100 animals back then, now there are 110.
  - than, in another place, where there were 10 animals before, we do not expect
    - 20 animals now, but rather
    - 11 animals per night.
* But, at the same time, we want to have mathematically additive effects like $y = y_0+a$
* Since $e^{y_0+a} = e^{y_0}e^a$ the exponential function can give us 
  - an additive formulation of a multiplicative effect.

## Simulating the data

```{r, include=FALSE}
set.seed(1)
```


```{r, title = "Create our simulated data", results='markup'}
rate # actually log(base rate)
eff # 1st element: 10YearsAgo, 2nd element: today
head(time) # not so very realistic...
expand_grid(rate=rate, eff=eff, time=time) %>% 
  mutate(cond = c("10YearsAgo", "today")[as.numeric(factor(eff))],
         eta = eff+rate, # still w/o offset
         exp_eta = exp(eta), # still w/o offset, rate of triggers (per hour)
         lambda = exp_eta*time, # =exp(eff)*exp(rate)*time <=> log(lambda) = log(time) + ... 
         resp = rpois(n(), lambda)) -> dta # draw from poisson distribution
```

```{r, echo=FALSE}
dta %>% 
  some(4) %>% 
  mutate(across(where(is.numeric), ~signif(., 4))) %>% 
  kbl %>% 
  kable_styling()
```

## Doing the model | not yet mixed! {.build .small .column}

```{r, row = F, title = "We use glm for unmixed models.", width=9}
m0 <- glm(resp~cond + offset(log(time)), data=dta, family = poisson)
summary(m0)
```


```{r, row = F, title = "The truth", width=3}
rate
eff[2]
```

### {.box-3}

That is very close to what we put into the model:

* The `(Intercept)` is the *estimate* for `rate` while 
* `condtoday` is the *estimate* for `eff[2]`. 

## fitted values

This quite obviously reprorduces our preset parameters.

```{r, include=T}
cc <- coefficients(summary(m0))[,1]
```

Diese Vorkommensrate wäre dann wieder mit der Zeit zu multiplizieren, um den für eine bestimmte Nacht vorhergesagten Wert der Auslösungen zu bekommen.

```{r}
dta %>% 
  mutate(fitted = fitted(m0), ## what the model gives us
         computed = exp(cc[1]+(cond=="today")*cc[2])*time) %>% 
  some(6)
```

It's quite straight forward so far. let's introduce random intercepts now.



## Acknowledgements { .column}

```{r include=F}
pkn <- sort(names(sessionInfo()$otherPkgs))
brk <- 17
```


### Packages used in this presentation {.box-6 .bg-yellow .small}

* `r paste(pkn[1:brk], pkn[1:brk], sep=", [@", collapse="]\n* " )`]


### {.box-6 .bg-yellow .small}

* `r paste(pkn[(brk+1):length(pkn)], pkn[(brk+1):length(pkn)], sep=", [@", collapse="]\n* " )`]

### Inspired by {.box-6 .bg-red}

technical enlightenment and substantial stimulus from @Krause2020

## References 

### {.col-12 .x-small}
