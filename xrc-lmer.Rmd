---
title: "Exercise for Linear General Mixed Models"
author: "Felix Golcher"
date: "`r format(Sys.time(), '%d %B, %Y %H:%M')`"
bibliography: [packages.bib]
output:
  bookdown::html_document2:
    toc: true
    toc_float: true
    toc_depth: 6
    number_sections: true
    fig_caption: yes
    table_caption: yes
    df_print: "paged"
---


```{r setup, include=FALSE}
## It's not necessary that you understand the code in this block.
## It's just convenient stuff and maybe nice to see.

## Some defaults how to render the output doc
knitr::opts_chunk$set(cache = TRUE, ## remember old computations, can _vastly_ reduce computation time.
                      autodep = TRUE, ## recompute dependent chunks
                      message = FALSE, ## do not write messages to the output document
                      warning = FALSE, ## dito with warnings
                      fig.width = (myfw <- 7), ## default width of figures
                      fig.height = myfw/2) ## make figures half as high as they are wide.
inputpath <- "data/" ## where to find the data
outputpath <- inputpath ## where to write

## you might want to change default contrasts
# options(contrasts = c(unordered = "contr.sum",
#                           ordered= "contr.poly"))

```

```{r packages, include=FALSE, cache=FALSE}
## maybe you don't _need_ all those:
library(readxl)
library(gridExtra)
# library(ggrepel)
library(tidyverse)
# library(ggplot2)
# library(emmeans)
library(Hmisc)
library(scales)
library(GGally)
library(qqplotr)
library(lme4)
library(lmerTest)
# library(optimx)
# library(nloptr)
library(effects)
# library(papaja)
library(sjPlot)
library(sjlabelled)
library(kableExtra)
library(knitr)
## if you don't want to render 0.001 as .001 remove this hook:
knit_hooks$set(inline =
  function (x) {
      if (is.numeric(x)) {
          x = round(x, getOption("digits"))
          if (x < 1 & x > -1) {
            x = sub("0\\.","\\.",as.character(x)) 
          }

      }
      paste(as.character(x), collapse = ", ")
      }
  )
## write a bib file for each loaded package:
## now you can cite package lme4 as @R-lme4.
## It's nice to cite packages: People get the cred they deserve and
## readers know what you were using.
write_bib(x = .packages(), file="packages.bib")
## plots light and white:
theme_set(theme_minimal())

## set som defaults
tab_model <- function(...){
  sjPlot::tab_model(collapse.ci = T,
                    linebreak = T,
                    digits.p = 3, # not smaller, since bug https://github.com/strengejacke/sjPlot/issues/789
                    show.intercept = F,
                    p.style = "numeric", # numeric seems broken.
                    show.icc = F,
                    show.obs = F,
                    show.ngroups = F,
                    ci.hyphen = ",",
                    ...)
}

# this semantics should exist, but what was it called again...
some <- function(dtx, length = 10){
    slice(dtx, as.integer(seq(1, n(), length = length)))
}
```


# Reading in the data

We have two data files

## Main data

```{r}
maindata <- read_excel("data/LengthInWords.xlsx")
maindata %>% 
  some() %>% 
  kbl() %>% 
  kable_styling()
```

* Subjects wrote texts. Each data point (line) represents one text written by one subjects.
* `TN_ID` is the subject id.
* `Situation` is one of 4 situations the subjects were confronted with. There were 2 more formal and 2 more informal ones. The design is not really $2\times 2$ since the two pairs cannot be matched.
* `Words` is the number of words the subjects produced in this situation.
* `Question` is the prompt the subjects were given.
* `Correctness_score` is a manual assessment of the texts.

## Meta data

```{r}
metadata <- read_excel("data/LengthInWords-meta.xlsx")
metadata %>% 
    some() %>% 
  kbl() %>% 
  kable_styling()
```

These variables are mostly selfexplaining. `GT_points` are points reached in some grammar test.

## Join main und meta data

```{r}
fulldata <- full_join(maindata, metadata)
stopifnot(nrow(fulldata) == nrow(maindata)) # We should have the same number of lines afterwards
stopifnot(ncol(fulldata) == ncol(maindata) + ncol(metadata) -1) # exactly on column for joining
```

# Analysis

## Research question

> Does the number of words differ between situations?

## Three variables describe something very similar

```{r, fig.cap="Funny situation: Two of the three pairs are clearly correlated, but the third pair much less."}
grid.arrange(
  ggplot(metadata, aes(GT_points, Abi))+
    geom_point()+
    geom_smooth(method="lm"),
  ggplot(fulldata, aes(GT_points, Correctness_score))+
    geom_jitter()+
    geom_smooth(method="lm"),
  ggplot(fulldata, aes(Abi, Correctness_score))+
    geom_jitter()+
    geom_smooth(method="lm"), nrow=1)
```


```{r, fig.cap="When we average over people we still have the same situation."}
my_fn <- function(data, mapping, method="lm", ...){
  p <- ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=method, ...)
  p
}
fulldata %>% 
  group_by(TN_ID) %>% 
  summarise(GT_points = first(GT_points),
            Abi = first(Abi),
            Correctness_score = mean(Correctness_score)) %>% 
  select(-TN_ID) %>% 
  ggpairs(lower = list(continuous = my_fn))
```

We will use only one of those variables. Otherwise there could be problems with interpreting the model.

## How do the data look like?

### Plot 1: Is there an effect?

**TASK** Produce a plot:

* that has *situation* on the X-axis
* and Words on the Y-axis
* underlay it with boxplots.

```{r, echo=FALSE, fig.cap="There seems to be an effect of *situation*; The data don't look normally distributed."}
fulldata %>% 
  ggplot(aes(Situation, Words#, col=Question
             ))+
  geom_boxplot()+
  geom_jitter(width = .01)#+
  # scale_y_sqrt()
```

### Plot 2: Distribution

**TASK** Plot the distribution of *Words*.

```{r}
fulldata %>% 
  ggplot(aes(Words))+
  geom_histogram()
```

### Plot 3: Transformations

QQ-plots show us how normally the data are distributed. Perfect data look like this:

```{r, fig.cap="QQ-Plot for perfectly normal data. You don't see those in the wild."}
tibble(x = rnorm(1e3)) %>% 
  ggplot(aes(sample = x))+
  geom_qq_band()+
  geom_qq()+
  geom_qq_line()
```

**TASK** Plot QQ-Plots for

* the untransformed data
* log-transformed data: $x^\prime = \log x$ (the standard)
* $\sqrt{~~}$-transformed data: $x^\prime = \sqrt{x}$ (strangely it sometimes works)
* inverse transformed data: $x^\prime = -1/x$ (good candidate for reaction time data)

```{r, include=FALSE}
fulldata %>% 
    group_by(Situation) %>% 
    mutate(Words = scale(Words)) %>% 
    ungroup() %>% 
   ggplot(aes(sample=Words))+
   geom_qq_line()+
   geom_qq()->gg7
```


```{r, fig.cap="Square root transformed data show pretty normally distributed.", echo=FALSE}
grid.arrange(gg7 + ggtitle("untransformed"),
             (gg7 +
               ggtitle("log transformed")) %+% 
               (fulldata %>% 
                  mutate(Words = log(Words)) %>% 
                  group_by(Situation) %>% 
                  mutate(Words = scale(Words)) %>% 
                  ungroup()),
             (gg7 +
               ggtitle("sqrt transformed")) %+% 
               (fulldata %>% 
                  mutate(Words = sqrt(Words)) %>% 
                  group_by(Situation) %>% 
                  mutate(Words = scale(Words)) %>% 
                  ungroup()),
             (gg7 +
               ggtitle("reverse transformed")) %+% 
               (fulldata %>% 
                  mutate(Words = -1/(Words)) %>% 
                  group_by(Situation) %>% 
                  mutate(Words = scale(Words)) %>% 
                  ungroup()),
             nrow = 2)
```

We did not look at residuals so far, which is what actually has to be normally distributed. But we got a glance that gave us a very good candidate.

## The model

Now we actually proceed with computing a mixed model. In reality we would do a lot more consistency checks on the data, more diagnostic plots and all that.

### Setup

**TASK** Set up a mixed model with `lmer` from package `lme4` [@R-lme4]. Use one of the three variables `Abi`, `GT_Points` and `Correctness_score` that somehow measure the knowledge of the subjects.

```{r}
w0 <- lmer(sqrt(Words) ~ Abi + Situation + (1|TN_ID), data=fulldata)
summary(w0)
```

### Assessment

#### Residual plot

**TASK** Compile a QQ-Plot for the residuals. Is it OK?

```{r, echo=FALSE, fig.cap="The Residuals look quite nice"}
data.frame(resid = resid(w0)) %>% 
  ggplot(aes(sample = scale(resid))) + ## qqband wants that... 
  geom_qq_band() +
  geom_qq()+
  geom_qq_line()
```

#### p-values

* The function `update` can and should be used to change things in a model.
* `anova` can be used to compare two nested models and to get a significance test.

```{r}
w0.reml <- update(w0, REML = T) # but anova does this automatically...
w0.reml.noabi <- update(w0.reml, .~. -Abi)
anova(w0.reml.noabi, w0.reml)
```

The result is very similar to what you saw in the model table:

```{r}
coefficients(summary(w0))["Abi",]
```

We can use the `drop1` function to remove one predictor at a time and run a significance test on it.

**TASK**  Do that with your model. Do also add the interaction between `Abi` and `situation` and check its significance. 

```{r}
drop1(update(w0, REML = F), test = "Chisq")
drop1(update(w0, .~. + Abi:Situation, REML = F), test = "Chisq")
```



```{r, fig.cap="Always look at what you did.", fig.width=8, fig.height=4}
plot(allEffects(w0))
```



```{r}
tab_model(w0)
```

### Change contrasts

How do default contrasts look like?

```{r}
(treat <- as.matrix(cbind(
  # offset
  c(1,0,0,0),
  # A vs B
  c(-1,1,0,0),
  # A vs C
  c(-1,0,1,0),
  # A vd D
  c(-1, 0, 0, 1))))
(taert <- solve(t(treat)))
aert <- taert[,-1]
colnames(aert) <- c("A vs B","A vs C", "A vs D")
aert == contr.treatment(4)
aert == contrasts(factor(fulldata$Situation))
```

Or, the other way round:

```{r}
t(solve(cbind(1,aert)))
```

But now we do not want to compare a default level to all others. That is only useful for experiments where you have a clear control level.

```{r}
t(solve(cbind(1, contr.sum(4))))
```

This gives us the grand mean as an intercept and pitches the first three factors against the other three.

This version of contrasts is quit readable. We can use it to easily construct contrasts that show whatever comparisons we want to see. Let's assume we want to compare 

* the formal conditions with each other
* the informal condition with each other
* the formal pair against the informal pair

**TASK** Set up those contrasts.

```{r}
(cm <- as.matrix(cbind(
  # offset
  1/4,
  # A vs B
  c(1,-1,0,0),
  # C vs D
  c(0,0,1,-1),
  # AB vd CD
  c(1/2, 1/2, -1/2, -1/2))))
cmm <- solve(t(cm))
cmn <- cmm[,-1]
colnames(cmn) <- c("Formal: A vs B","Informal: C vs D", "Formal vs Informal")
```

**TASK** change the data so that the contrasts are taken to be the contrasts used for *situation*


```{r}
fulldata %>% 
  mutate(Situation = C(factor(Situation), contr = cmn)) -> contrdata
```

**TASK** rerun your model.


```{r}
w1 <- update(w0, data = contrdata)
```

```{r}
tab_model(w1)
```

```{r}
plot(allEffects(w1)[c(1,2)])
```

# Acknowledgement

Thanks to Milena Kühnast for the nice data set.

# packages we used

`r paste0("@R-",setdiff(.packages(), c("datasets","grDevices", "graphics", "methods", "stats", "utils")), collapse = ", ")`
